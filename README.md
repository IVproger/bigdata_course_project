# ğŸ—ï¸ Big-Data Salary-Prediction Pipeline

End-to-end Spark / Hadoop project that ingests **Kaggle job-description data**, turns it into an **analytics-ready Hive warehouse**, runs **Spark-SQL EDA**, trains & tunes **Spark-ML regression models**, and surfaces everything in an **Apache Superset dashboard**.

High End architecture of the project
```mermaid
flowchart TB
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    %% Stage I â€” Data Collection
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph STAGE1["Stage I â€” Data Collection"]
        direction LR
        Kaggle["Kaggle CLI<br/>(job-descriptions.csv)"] --> PostgreSQL["PostgreSQL"]
        PostgreSQL --> Sqoop["Sqoop Import"]
        Sqoop --> HDFS1["HDFS<br/>Avro (+ schema)"]
    end

    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    %% Stage II â€” Data Warehouse & EDA
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph STAGE2["Stage II â€” Data Warehouse & EDA"]
        direction LR
        Hive["Hive Externals<br/>(partitioned & bucketed)"] --> SparkSQL["Spark SQL<br/>(6 analyses)"]
    end

    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    %% Stage III â€” Predictive Analytics
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph STAGE3["Stage III â€” Predictive Analytics"]
        direction TB
        Preproc["Data Preprocessing<br/>(Spark DataFrame ops)"] --> SparkML["ML Modelling<br/>(Spark ML Pipeline)"]
        SparkML --> LR["Linear Regression"]
        SparkML --> GBT["Gradient-Boosted Trees"]
    end

    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    %% Stage IV â€” Presentation & Delivery
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    subgraph STAGE4["Stage IV â€” Presentation & Delivery"]
        direction LR
        HiveExt["Hive Externals<br/>(metrics & predictions)"] --> Superset["Apache Superset<br/>Dashboards"]
    end

    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    %% Cross-stage flow
    %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    HDFS1 --> Hive            
    SparkSQL --> Preproc        
    LR --> HiveExt       
    GBT --> HiveExt

```
---

## âœ¨ Key Features
- **One-click pipeline.** 4 bash stages or a single `main.sh`.
- **Optimised Hive layout.** Avro + partitioning (`work_type`) + bucketing (`preference`) for low scan cost.
- **Scalable ML.** Linear Regression vs. Gradient-Boosted Trees with 3-fold CV, persisted in HDFS.
- **Metrics at a glance.** KL-divergence, RMSE, RÂ² and hyper-parameter grids ready for BI tools.
- **Dashboard ready.** External Hive tables expose CSV outputs directly to Superset.

---

## ğŸ—‚ï¸ Repository Layout
```
â”œâ”€â”€ data/            # Raw download + ML data splits (synced from HDFS)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ img/         # â† Put your screenshots here
â”‚   â””â”€â”€ report_*.md  # In-depth reports for each stage
â”œâ”€â”€ models/          # Trained Spark-ML models
â”œâ”€â”€ output/          # Avro schemas, EDA CSVs, predictions, evaluation logs
â”œâ”€â”€ scripts/         # Bash & Python automation
â”œâ”€â”€ sql/             # PostgreSQL & Hive DDL / DML
â””â”€â”€ .venv/           # Project-scoped virtualenv
```

---

## âš¡ Quick Start

> **Prerequisites**  
> Python 3.11 â€¢ Hadoop 3 â€¢ Hive 3 â€¢ Spark 3.5 â€¢ Sqoop 1.4 â€¢ PostgreSQL 15 â€¢ Kaggle CLI

```bash
# clone & bootstrap
git clone https://github.com/<your-org>/big-data-salary.git && cd big-data-salary
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# store secrets
echo "POSTGRES_PASSWORD=********" > secrets/.psql.pass

# full run (â‰ˆ 30 min on 4-node cluster)
bash main.sh
```

Each stage can be invoked separately if you prefer:

```bash
bash scripts/stage1.sh   # ingest â†’ PostgreSQL â†’ HDFS
bash scripts/stage2.sh   # Hive warehouse + Spark-SQL EDA
bash scripts/stage3.sh   # Spark-ML training & tuning
bash scripts/stage4.sh   # metrics â†’ Hive for BI
```

---

## ğŸ” Stage Breakdown

| Stage | What happens | Key outputs |
|-------|--------------|-------------|
| **1 Data Collection** | Kaggle â†’ PostgreSQL â†’ Sqoop Avro in HDFS | `warehouse/*.avro` |
| **2 Warehouse & EDA** | Partitioned + bucketed Hive table, 6 Spark-SQL analyses | `output/q*_results.csv` |
| **3 Predictive ML** | Linear vs. GBT, 3-fold CV, log-salary target | `models/**`, `output/model*_predictions.csv` |
| **4 Presentation** | KL divergence, Hive externals for Superset | `output/kl_divergence.csv` |

Details live in [`docs/report_*.md`](docs/) for auditors and graders.

---

## ğŸ“Š Dashboard Preview

<p align="center">
  <img src="docs/img/superset_overview.png" width="700" alt="Superset overview dashboard"/>
</p>

<!-- Repeat for any other EDA screenshots -->
<p align="center">
  <img src="docs/img/salary_dist_by_role.png" width="350" alt="Salary distribution by role"/>
  <img src="docs/img/model_rmse_comparison.png" width="350" alt="Model RMSE comparison"/>
</p>

---

## ğŸ”¬ Results
| Model | RMSE (log) | RÂ² (log) | KL-Div. (salary) |
|-------|------------|----------|------------------|
| Linear Reg. | 0.273 | 0.87 | 0.052 |
| GBT | **0.201** | **0.93** | **0.039** |

â†’ GBT shows a 26 % RMSE reduction and better KL divergence, indicating tighter fit on the heavy-tailed salary distribution.

---

## ğŸ› ï¸ Development Tips
```bash
# unit tests
pytest -q

# lint / style
ruff check .
black --check .

# regenerate architecture diagram (draw.io export)
docs/img/architecture_overview.png
```

---

## ğŸ¤ Contributing
Pull requests welcome! Please open an issue first to discuss major changes.

1. Fork âœ create feature branch (`git checkout -b feat/my-feature`)  
2. Commit + push (`git commit -m "feat: add â€¦"` â†’ `git push origin`)  
3. Open PR â†’ pass CI.

---

## ğŸ“„ License
Distributed under the MIT License. See [`LICENSE`](LICENSE) for details.

---

## ğŸ™ Acknowledgements
- **Kaggle** for the open job-descriptions dataset  
- **Apache Software Foundation** for the Hadoop ecosystem  
- University **Big-Data Engineering** course staff for project guidance

---

> _Happy crunching â€” and may your HDFS never fill up!_
