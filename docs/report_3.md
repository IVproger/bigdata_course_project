# Stage III - Predictive Data Analytics Report

## Overview
This report outlines the implementation of Stage III of the big data pipeline project, focusing on predictive data analytics using Apache Spark ML. The objective was to build, train, evaluate, and tune machine learning models to predict the average salary (`salary_avg`) based on the features extracted and processed from the job descriptions dataset in the previous stages.

## Dataset Description
The input for this stage was the preprocessed data generated by `scripts/data_predprocessing.py` and stored in HDFS at `project/data/train` and `project/data/test` as JSON files. Each record contains a `features` vector (resulting from numerical scaling, one-hot encoding, TF-IDF, etc.) and a `label` (the `salary_avg`).

## Implementation Steps

### 1. Data Loading (`scripts/ml_modeling.py`)
- A Spark session was initialized with Hive support enabled.
- The preprocessed training and testing data were loaded from the HDFS JSON files (`project/data/train`, `project/data/test`).
- An explicit schema (`StructType([StructField("features", VectorUDT(), True), StructField("label", DoubleType(), True)])`) was provided during loading to ensure robustness and prevent schema inference issues.
- The loaded DataFrames (`train_data`, `test_data`) were cached for improved performance during iterative model training and tuning.

### 2. ML Modeling (`scripts/ml_modeling.py`)
Two regression models were selected to predict the average salary:

1.  **Linear Regression:** A standard linear model.
2.  **Gradient-Boosted Tree (GBT) Regressor:** An ensemble tree-based model.

For both models, the following steps were performed:

- **Model Initialization:** The respective regressors (`LinearRegression`, `GBTRegressor`) were instantiated with `featuresCol="features"` and `labelCol="label"`.
- **Hyperparameter Tuning Setup:**
    - `ParamGridBuilder` was used to define a grid of hyperparameters to search for each model:
        - *Linear Regression:* `regParam`, `elasticNetParam`, `aggregationDepth`.
        - *GBT Regressor:* `maxDepth`, `maxIter`, `stepSize`.
    - `CrossValidator` was configured to tune the models using 3-fold cross-validation on the `train_data`. `RMSE` was used as the evaluation metric for selecting the best hyperparameters.
- **Training and Best Model Selection:** The `CrossValidator` estimator was fitted to the `train_data`. The best model found during cross-validation was selected (`model1` for Linear Regression, `model2` for GBT).
- **Prediction:** The best model (`model1`, `model2`) was used to make predictions on the held-out `test_data`.
- **Evaluation:** The performance of the best model was evaluated on the `test_data` using both Root Mean Squared Error (`RMSE`) and R-squared (`R2`) metrics via `RegressionEvaluator`.

### 3. Model Comparison (`scripts/ml_modeling.py`)
- The evaluation results (RMSE and R2) for both `model1` and `model2` on the test data were collected.
- A Spark DataFrame was created to compare the performance of the two models side-by-side.

**Model Comparison Results:**
```
+------------------+------------------+--------------------+
|Model_Type        |RMSE              |R2                  |
+------------------+------------------+--------------------+
|LinearRegression  |7529.813526064228 |3.497925155993009E-5|
|GBTRegressor      |7529.786017965478 |4.228543150641695E-5|
+------------------+------------------+--------------------+
```

The results show very similar performance between the tuned Linear Regression and GBT models on the test set, with GBT having a slightly lower RMSE and slightly higher R2, though the R2 values are extremely close to zero, suggesting neither model explains much of the variance in the target variable with the current feature set and preprocessing.

### 4. Saving Models and Results (`scripts/ml_modeling.py` & `scripts/stage3.sh`)
- The best trained models (`model1`, `model2`) were saved to HDFS in the `project/models/` directory (`model1`, `model2`).
- The predictions generated by each model on the test data were saved as single-partition CSV files (with headers) to HDFS in `project/output/` (`model1_predictions.csv`, `model2_predictions.csv`).
- The model comparison DataFrame was saved as a single-partition CSV file (with headers) to HDFS in `project/output/evaluation.csv`.
- The `stage3.sh` script handles downloading these artifacts from HDFS to the local `models/` and `output/` directories after the Spark jobs complete.

### 5. Automation and Quality Check (`scripts/stage3.sh`)
- The entire Stage 3 process (preprocessing and modeling) is automated by the `scripts/stage3.sh` script.
- It ensures the necessary HDFS directories are created and cleaned.
- It executes the `data_predprocessing.py` and `ml_modeling.py` scripts using `spark-submit` configured for the YARN cluster.
- It downloads all necessary outputs (data, models, predictions, evaluation) from HDFS to the local project structure.
- Includes `pylint` checks (using `.pylintrc`) for both Python scripts involved in this stage to ensure code quality.

## Technical Choices

### Hyperparameter Tuning: CrossValidator
- `CrossValidator` with `numFolds=3` was used for hyperparameter tuning as required.
- It performs K-fold cross-validation (here, 3 folds) by splitting the training data into K smaller sets.
- For each hyperparameter combination, it trains on K-1 folds and validates on the remaining fold.
- The performance metric (RMSE) is averaged across the folds for each parameter combination.
- The parameters yielding the best average performance are chosen.
- This approach provides a more robust estimate of model performance on unseen data compared to a single train-validation split, albeit at a higher computational cost.

### Evaluation Metrics: RMSE and R2
- **RMSE (Root Mean Squared Error):** Provides an interpretable measure of the average magnitude of the prediction errors in the units of the target variable (salary). Lower is better.
- **R2 (R-squared):** Indicates the proportion of the variance in the target variable that is predictable from the features. A value closer to 1 indicates a better fit, while a value near 0 suggests the model does not explain much variance.

### Models: Linear Regression & GBTRegressor
- **Linear Regression:** Chosen as a baseline model due to its simplicity, interpretability, and computational efficiency.
- **GBTRegressor:** Selected as a more complex, potentially higher-performing model. Gradient Boosting builds trees sequentially, with each new tree trying to correct the errors of the previous ones, often leading to good predictive accuracy.

## Conclusion
Stage III successfully implemented a predictive modeling pipeline using Spark ML. It loaded preprocessed data, trained and tuned both Linear Regression and GBT Regressor models using 3-fold Cross-Validation, evaluated their performance on a held-out test set, and compared the results. The best models, their predictions, and the evaluation comparison were saved to HDFS and subsequently downloaded locally. The entire process is automated via the `stage3.sh` script, including code quality checks with Pylint. While the models' predictive performance (indicated by low R2 values) suggests further feature engineering or model exploration might be needed for better salary prediction, the pipeline itself fulfills the requirements of this stage. 