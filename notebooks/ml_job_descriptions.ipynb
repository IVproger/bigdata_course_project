{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and SparkSession setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/23 19:49:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/23 19:49:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/23 19:49:03 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/04/23 19:49:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Add here your team number\n",
    "team = 14\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(f\"Team {team} - spark ML Job Descriptions\")\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.submit.deployMode\", \"client\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Hive databases and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available databases:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/apps/hive/warehouse'), Database(name='retake1', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/retakedb1'), Database(name='root_db', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/root/root_db'), Database(name='show', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/data2'), Database(name='team0_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team0/project/hive/warehouse'), Database(name='team11_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team11/project/hive/warehouse'), Database(name='team12_hive_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team12/project/hive/warehouse'), Database(name='team13_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team13/project/hive/warehouse'), Database(name='team14_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team14/project/hive/warehouse'), Database(name='team15_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team15/project/hive/warehouse'), Database(name='team16_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team16/project/hive/warehouse'), Database(name='team17_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team17/project/hive/warehouse'), Database(name='team18_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team18/project/hive/warehouse'), Database(name='team19_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team19/project/hive/warehouse'), Database(name='team1_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team1/project/hive/warehouse'), Database(name='team20_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team20/project/hive/warehouse'), Database(name='team21_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team21/project/hive/warehouse'), Database(name='team22_projectbd', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team22/project/hive/warehouse'), Database(name='team22_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team22/project/hive/warehouse'), Database(name='team23_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/hive/warehouse'), Database(name='team24_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team24/project/hive/warehouse'), Database(name='team25_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team25/project/hive/warehouse'), Database(name='team26_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team26/project/hive/warehouse'), Database(name='team27_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team27/project/hive/warehouse'), Database(name='team28_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team28/project/hive/warehouse'), Database(name='team29_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team29/project/hive/warehouse'), Database(name='team2_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team2/project/hive/warehouse'), Database(name='team30_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team30/project/hive/warehouse'), Database(name='team31_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team31/project/hive/warehouse'), Database(name='team34_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team34/project/hive/warehouse'), Database(name='team36_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/project_table'), Database(name='team36db', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team36/hive_db'), Database(name='team37_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team37/project/hive/warehouse'), Database(name='team38_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team38/project/hive/warehouse'), Database(name='team39_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team39/project/hive/warehouse'), Database(name='team3_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team3/project/hive/warehouse'), Database(name='team4_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team4/project/hive/warehouse'), Database(name='team5_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team5/project/hive/warehouse'), Database(name='team6_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/project/hive/warehouse'), Database(name='team7_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team7/project/hive/warehouse'), Database(name='team8_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team8/project/hive/warehouse'), Database(name='team9_projectdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team9/project/hive/warehouse'), Database(name='testdb', catalog='spark_catalog', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/apps/hive/warehouse/testdb.db')]\n",
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|team12_hive_proje...|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "|    team22_projectbd|\n",
      "|    team22_projectdb|\n",
      "|    team23_projectdb|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Listing available databases:\")\n",
    "print(spark.catalog.listDatabases())\n",
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing tables in team14_projectdb:\n",
      "[Table(name='job_descriptions_part', catalog='spark_catalog', namespace=['team14_projectdb'], description=None, tableType='EXTERNAL', isTemporary=False)]\n",
      "+----------------+--------------------+-----------+\n",
      "|       namespace|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|team14_projectdb|job_descriptions_...|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = 'team14_projectdb'\n",
    "print(f\"\\nListing tables in {db}:\")\n",
    "print(spark.catalog.listTables(db))\n",
    "spark.sql(f\"USE {db};\")\n",
    "spark.sql(\"SHOW TABLES;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- job_id: long (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      " |-- qualifications: string (nullable = true)\n",
      " |-- salary_range: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: decimal(9,6) (nullable = true)\n",
      " |-- longitude: decimal(9,6) (nullable = true)\n",
      " |-- company_size: integer (nullable = true)\n",
      " |-- job_posting_date: date (nullable = true)\n",
      " |-- contact_person: string (nullable = true)\n",
      " |-- preference: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- job_portal: string (nullable = true)\n",
      " |-- job_description: string (nullable = true)\n",
      " |-- benefits: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      " |-- responsibilities: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_profile: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = 'job_descriptions_part'\n",
    "df = spark.read.format(\"avro\").table(f'{db}.{table_name}')\n",
    "print(\"\\nDataframe schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 19:50:47 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------+--------------+------------+------------+----------------------------+----------+----------+------------+----------------+---------------+----------+--------------------+------------------------+--------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|id     |job_id          |experience   |qualifications|salary_range|location    |country                     |latitude  |longitude |company_size|job_posting_date|contact_person |preference|contact             |job_title               |role                            |job_portal  |job_description                                                                                                                                                                                               |benefits                                                                                                                                              |skills                                                                                                                                                                                                                                      |responsibilities                                                                                                                                                                              |company_name        |company_profile                                                                                                                                                                                              |work_type|\n",
      "+-------+----------------+-------------+--------------+------------+------------+----------------------------+----------+----------+------------+----------------+---------------+----------+--------------------+------------------------+--------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|1173529|364241521459786 |4 to 11 Years|BA            |$62K-$118K  |Antananarivo|Madagascar                  |-18.879200|46.845100 |121206      |2022-01-31      |Calvin Thompson|Female    |472.993.7608x09238  |Project Coordinator     |Construction Project Coordinator|Jobs2Careers|Construction Project Coordinators assist in managing construction projects, handling documentation, budget tracking, and coordinating subcontractors.                                                         |{'Casual Dress Code, Social and Recreational Activities, Employee Referral Programs, Health and Wellness Facilities, Life and Disability Insurance'}  |Construction project management Building codes and regulations knowledge Budgeting and cost control Construction scheduling Contractor and subcontractor coordination                                                                       |Coordinate construction projects, including permits, subcontractors, and materials. Monitor project progress and timelines. Assist in budgeting and cost control.                             |Sysco               |{\"Sector\":\"Food and Beverage\",\"Industry\":\"Wholesalers: Food and Grocery\",\"City\":\"Houston\",\"State\":\"Texas\",\"Zip\":\"77077\",\"Website\":\"www.sysco.com\",\"Ticker\":\"SYY\",\"CEO\":\"Kevin P. Hourican\"}                  |Intern   |\n",
      "|1173509|2336439403007876|5 to 12 Years|M.Tech        |$57K-$125K  |Kinshasa    |Democratic Republic Of Congo|-4.038300 |21.758700 |56920       |2022-11-19      |Tracy Johnson  |Female    |821.227.4997x0320   |Database Developer      |SQL Database Developer          |LinkedIn    |SQL Database Developers design, implement, and maintain relational databases using SQL (Structured Query Language). They write queries, optimize database performance, and ensure data integrity and security.|{'Childcare Assistance, Paid Time Off (PTO), Relocation Assistance, Flexible Work Arrangements, Professional Development'}                            |SQL (Structured Query Language) Database design Query optimization Data modeling Database maintenance Problem-solving skills                                                                                                                |Design, develop, and maintain SQL databases, ensuring data integrity and performance. Write complex SQL queries and stored procedures. Troubleshoot database issues and optimize queries.     |Andersons           |{\"Sector\":\"Agriculture\",\"Industry\":\"Food Production\",\"City\":\"Maumee\",\"State\":\"Ohio\",\"Zip\":\"43537\",\"Website\":\"www.andersonsinc.com\",\"Ticker\":\"ANDE\",\"CEO\":\"Patrick E. Bowe\"}                                  |Intern   |\n",
      "|1173480|293567730732644 |5 to 12 Years|M.Tech        |$60K-$126K  |Lisbon      |Portugal                    |39.399900 |-8.224500 |45443       |2023-03-05      |Diana Sanders  |Female    |+1-595-207-2116x4110|Teacher                 |Special Education Teacher       |FlexJobs    |A Special Education Teacher works with students who have disabilities, tailoring instruction to meet their unique needs and ensuring they have access to a quality education.                                 |{'Life and Disability Insurance, Stock Options or Equity Grants, Employee Recognition Programs, Health Insurance, Social and Recreational Activities'}|Special education strategies Individualized education plans (IEPs) Behavior management techniques Collaboration with support staff Special education laws and regulations knowledge                                                         |Provide tailored instruction and support to students with special needs. Develop and implement Individualized Education Plans (IEPs). Collaborate with parents, therapists, and support staff.|Aditya Birla Capital|{\"Sector\":\"Financial Services\",\"Industry\":\"Financial Services\",\"City\":\"Mumbai\",\"State\":\"Maharashtra\",\"Zip\":\"400013\",\"Website\":\"https://www.adityabirlacapital.com/\",\"Ticker\":\"ABCAP\",\"CEO\":\"Ajay Srinivasan\"}|Intern   |\n",
      "|1173456|1534081140901895|0 to 15 Years|MCA           |$55K-$100K  |Sofia       |Bulgaria                    |42.733900 |25.485800 |124743      |2022-11-03      |Paul Lyons     |Female    |+1-922-681-4211     |Sales Consultant        |Sales Advisor                   |Monster     |Sales Advisors provide expert advice to customers about products or services. They help customers make informed purchase decisions, answer questions, and offer solutions tailored to individual needs.       |{'Employee Referral Programs, Financial Counseling, Health and Wellness Facilities, Casual Dress Code, Flexible Spending Accounts (FSAs)'}            |Sales strategies Customer relationship management Sales techniques Product knowledge Client acquisition Negotiation skills                                                                                                                  |Provide expert advice and guidance to customers on products or services. Understand customer needs and recommend appropriate solutions. Achieve sales targets and quotas.                     |Bank of China       |{\"Sector\":\"Banking\",\"Industry\":\"Banking & Financial Services\",\"City\":\"Beijing\",\"State\":\"Beijing\",\"Zip\":\"100033\",\"Website\":\"https://www.boc.cn/en/\",\"Ticker\":\"601988.SS\",\"CEO\":\"Chen Yuan\"}                   |Intern   |\n",
      "|1173441|2559112497090360|4 to 12 Years|MCA           |$56K-$106K  |Kuala Lumpur|Malaysia                    |4.210500  |101.975800|37463       |2022-08-25      |David Mcgee    |Female    |(289)392-5660       |Administrative Assistant|Data Entry Specialist           |The Muse    |Accurately input and maintain data in databases or spreadsheets, ensuring data integrity and organization.                                                                                                    |{'Life and Disability Insurance, Stock Options or Equity Grants, Employee Recognition Programs, Health Insurance, Social and Recreational Activities'}|Data entry accuracy and speed Database software (e.g., Microsoft Access, Excel) Attention to detail Time management Organization Multitasking Keyboarding skills Data verification Problem-solving Communication skills Computer proficiency|Accurately enter and maintain data in databases or spreadsheets. Perform data verification and quality checks. Assist with data analysis and reporting.                                       |Ambuja Cements      |{\"Sector\":\"Cement & Aggregates\",\"Industry\":\"Cement and Concrete\",\"City\":\"Mumbai\",\"State\":\"Maharashtra\",\"Zip\":\"400001\",\"Website\":\"https://www.ambujacement.com/\",\"Ticker\":\"AMBUJACEM\",\"CEO\":\"Neeraj Akhoury\"} |Intern   |\n",
      "+-------+----------------+-------------+--------------+------------+------------+----------------------------+----------+----------+------------+----------------+---------------+----------+--------------------+------------------------+--------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample data:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows before cleaning: 1615940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal rows before cleaning: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows after removing nulls: 1615940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "df = df.na.drop()\n",
    "print(f\"Total rows after removing nulls: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse salary_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_range(salary):\n",
    "    try:\n",
    "        if salary is None:\n",
    "            return (None, None)\n",
    "        \n",
    "        # Remove the currency symbol and 'K'\n",
    "        # Example: \"$59K-$99K\" -> [\"59\", \"99\"]\n",
    "        salary = salary.replace('$', '')\n",
    "        parts = salary.split('-')\n",
    "        \n",
    "        min_salary = float(parts[0].replace('K', '')) * 1000 if 'K' in parts[0] else float(parts[0])\n",
    "        max_salary = float(parts[1].replace('K', '')) * 1000 if 'K' in parts[1] else float(parts[1])\n",
    "        \n",
    "        return (min_salary, max_salary)\n",
    "    except:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_salary_udf = F.udf(extract_salary_range, StructType([\n",
    "    StructField(\"min\", DoubleType(), True),\n",
    "    StructField(\"max\", DoubleType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"salary_parsed\", extract_salary_udf(F.col(\"salary_range\")))\n",
    "df = df.withColumn(\"salary_min\", F.col(\"salary_parsed.min\"))\n",
    "df = df.withColumn(\"salary_max\", F.col(\"salary_parsed.max\"))\n",
    "df = df.withColumn(\"salary_avg\", (F.col(\"salary_min\") + F.col(\"salary_max\"))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null salary values\n",
    "df = df.filter(F.col(\"salary_min\").isNotNull() & F.col(\"salary_max\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|salary_avg|\n",
      "+----------+\n",
      "|90000.0   |\n",
      "|91000.0   |\n",
      "|93000.0   |\n",
      "|77500.0   |\n",
      "|81000.0   |\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"salary_avg\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experience(exp_str):\n",
    "    try:\n",
    "        if exp_str is None:\n",
    "            return (None, None)\n",
    "        \n",
    "        # Find all numbers in the string\n",
    "        numbers = re.findall(r'\\d+', exp_str)\n",
    "        \n",
    "        if len(numbers) >= 2:\n",
    "            min_exp = int(numbers[0])\n",
    "            max_exp = int(numbers[1])\n",
    "            return (min_exp, max_exp)\n",
    "        elif len(numbers) == 1:\n",
    "            # If only one number, use it for both min and max\n",
    "            exp = int(numbers[0])\n",
    "            return (exp, exp)\n",
    "        else:\n",
    "            return (None, None)\n",
    "    except:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_experience_udf = F.udf(extract_experience, StructType([\n",
    "    StructField(\"min\", IntegerType(), True),\n",
    "    StructField(\"max\", IntegerType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"experience_parsed\", extract_experience_udf(F.col(\"experience\")))\n",
    "df = df.withColumn(\"experience_min\", F.col(\"experience_parsed.min\"))\n",
    "df = df.withColumn(\"experience_max\", F.col(\"experience_parsed.max\"))\n",
    "df = df.withColumn(\"experience_avg\", (F.col(\"experience_min\") + F.col(\"experience_max\"))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|experience_avg|\n",
      "+--------------+\n",
      "|7.5           |\n",
      "|8.5           |\n",
      "|8.5           |\n",
      "|7.5           |\n",
      "|8.0           |\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"experience_avg\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process company_profile JSON field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to check if a string is valid JSON\n",
    "def is_valid_json(s):\n",
    "    try:\n",
    "        json.loads(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_json_udf = F.udf(is_valid_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean company_profile JSON field\n",
    "df = df.withColumn(\"is_valid_json\", is_valid_json_udf(F.col(\"company_profile\")))\n",
    "df = df.filter(F.col(\"is_valid_json\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows after json validation: 1608618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows after json validation: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean JSON string by replacing single quotes with double quotes\n",
    "df = df.withColumn(\"company_profile_cleaned\", F.regexp_replace(F.col(\"company_profile\"), \"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for company profile\n",
    "company_profile_schema = StructType([\n",
    "    StructField(\"Sector\", StringType(), True),\n",
    "    StructField(\"Industry\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", StringType(), True),\n",
    "    StructField(\"Website\", StringType(), True),\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"CEO\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON into separate columns\n",
    "df = df.withColumn(\"company_profile_parsed\", F.from_json(F.col(\"company_profile_cleaned\"), company_profile_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract each field from the parsed struct\n",
    "df = df.withColumn(\"sector\", F.col(\"company_profile_parsed.Sector\"))\n",
    "df = df.withColumn(\"industry\", F.col(\"company_profile_parsed.Industry\"))\n",
    "df = df.withColumn(\"company_city\", F.col(\"company_profile_parsed.City\"))\n",
    "df = df.withColumn(\"company_state\", F.col(\"company_profile_parsed.State\"))\n",
    "df = df.withColumn(\"company_zip\", F.col(\"company_profile_parsed.Zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|company_state|\n",
      "+-------------+\n",
      "|Telangana    |\n",
      "|N/A          |\n",
      "|Pennsylvania |\n",
      "|Texas        |\n",
      "|Illinois     |\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"company_state\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is experimental feature FIX ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"is_public\", \n",
    "    F.when(\n",
    "        F.col(\"company_profile_parsed.Ticker\").isNotNull() & (F.length(F.col(\"company_profile_parsed.Ticker\")) > 0), \n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer CEO gender\n",
    "def infer_gender(name):\n",
    "    if name is None or name.strip() == \"\":\n",
    "        return \"unknown\"\n",
    "    # Very simple gender inference based on first name\n",
    "    # This is a simplified approach - consider using a proper gender detection library in production\n",
    "    male_names = {\"john\", \"david\", \"michael\", \"robert\", \"james\", \"william\", \"mark\", \"richard\", \"thomas\", \"charles\", \n",
    "                 \"steven\", \"kevin\", \"joseph\", \"brian\", \"jeff\", \"scott\", \"mike\", \"paul\", \"dan\", \"chris\", \"tim\", \"greg\"}\n",
    "    female_names = {\"mary\", \"patricia\", \"jennifer\", \"linda\", \"elizabeth\", \"barbara\", \"susan\", \"jessica\", \"sarah\", \n",
    "                   \"karen\", \"lisa\", \"nancy\", \"betty\", \"margaret\", \"sandra\", \"ashley\", \"kimberly\", \"emily\", \"donna\", \n",
    "                   \"michelle\", \"carol\", \"amanda\", \"melissa\", \"deborah\", \"stephanie\"}\n",
    "    \n",
    "    first_name = name.split()[0].lower()\n",
    "    if first_name in male_names:\n",
    "        return \"male\"\n",
    "    elif first_name in female_names:\n",
    "        return \"female\"\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_gender_udf = F.udf(infer_gender, StringType())\n",
    "df = df.withColumn(\"ceo_gender\", infer_gender_udf(F.col(\"company_profile_parsed.Ceo\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary columns\n",
    "df = df.drop(\"is_valid_json\", \"company_profile_cleaned\", \"company_profile_parsed\", \"experience_parsed\", \"salary_parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|ceo_gender|\n",
      "+----------+\n",
      "|unknown   |\n",
      "|female    |\n",
      "|male      |\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"ceo_gender\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process benefits field ???? (FIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean benefits field\n",
    "df = df.withColumn(\"benefits_cleaned\", \n",
    "                   F.regexp_replace(F.regexp_replace(F.col(\"benefits\"), \"\\\\{\\\\\\'|\\\\\\'\\\\\\'\\\\\\\\'\", \"\"), \",\\\\s\", \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of benefits listed\n",
    "df = df.withColumn(\"benefits_count\", F.size(F.split(F.col(\"benefits_cleaned\"), \",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common benefits as binary flags\n",
    "common_benefits = [\"health insurance\", \"dental\", \"vision\", \"401k\", \"retirement\", \"pto\", \"paid time off\", \n",
    "                   \"flexible\", \"remote\", \"bonus\", \"education\", \"training\", \"insurance\", \"life insurance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for benefit in common_benefits:\n",
    "    df = df.withColumn(f\"has_{benefit.replace(' ', '_')}\", \n",
    "                        F.when(F.lower(F.col(\"benefits\")).contains(benefit), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|benefits_cleaned                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Casual Dress Code,Social and Recreational Activities,Employee Referral Programs,Health and Wellness Facilities,Life and Disability Insurance'}  |\n",
      "|Childcare Assistance,Paid Time Off (PTO),Relocation Assistance,Flexible Work Arrangements,Professional Development'}                            |\n",
      "|Life and Disability Insurance,Stock Options or Equity Grants,Employee Recognition Programs,Health Insurance,Social and Recreational Activities'}|\n",
      "|Employee Referral Programs,Financial Counseling,Health and Wellness Facilities,Casual Dress Code,Flexible Spending Accounts (FSAs)'}            |\n",
      "|Life and Disability Insurance,Stock Options or Equity Grants,Employee Recognition Programs,Health Insurance,Social and Recreational Activities'}|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"benefits_cleaned\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|benefits_count|\n",
      "+--------------+\n",
      "|5             |\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"benefits_count\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process skills and responsibilities fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of skills and responsibilities listed\n",
    "df = df.withColumn(\"skills_count\", F.size(F.split(F.col(\"skills\"), \" \")))\n",
    "df = df.withColumn(\"responsibilities_count\", F.size(F.split(F.col(\"responsibilities\"), \"\\\\.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|skills_count|\n",
      "+------------+\n",
      "|31          |\n",
      "|34          |\n",
      "|28          |\n",
      "|27          |\n",
      "|26          |\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"skills_count\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|skills_count|\n",
      "+------------+\n",
      "|31          |\n",
      "|34          |\n",
      "|28          |\n",
      "|27          |\n",
      "|26          |\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"skills_count\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process job_posting_date (temporal features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day features from job_posting_date\n",
    "df = df.withColumn(\"job_posting_year\", F.year(F.col(\"job_posting_date\")))\n",
    "df = df.withColumn(\"job_posting_month\", F.month(F.col(\"job_posting_date\")))\n",
    "df = df.withColumn(\"job_posting_day\", F.dayofmonth(F.col(\"job_posting_date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cyclical features for month and day\n",
    "df = df.withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"job_posting_month\") / 12))\n",
    "df = df.withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"job_posting_month\") / 12))\n",
    "df = df.withColumn(\"day_sin\", F.sin(2 * math.pi * F.col(\"job_posting_day\") / 31))\n",
    "df = df.withColumn(\"day_cos\", F.cos(2 * math.pi * F.col(\"job_posting_day\") / 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|month_sin              |\n",
      "+-----------------------+\n",
      "|-1.0                   |\n",
      "|1.0                    |\n",
      "|0.8660254037844386     |\n",
      "|-2.4492935982947064E-16|\n",
      "|-0.4999999999999997    |\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"month_sin\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process geospatial data (latitude and longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert latitude and longitude to ECEF (Earth-Centered, Earth-Fixed) coordinates\n",
    "# WGS84 ellipsoid constants\n",
    "a = 6378137.0  # semi-major axis in meters\n",
    "b = 6356752.314245  # semi-minor axis in meters\n",
    "f = (a - b) / a  # flattening\n",
    "e_sq = f * (2 - f)  # eccentricity squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_to_ecef(lat, lon, alt=0):\n",
    "    # Convert geodetic coordinates to ECEF coordinates\n",
    "    # lat, lon in degrees, alt in meters\n",
    "    lat_rad = lat * math.pi / 180.0\n",
    "    lon_rad = lon * math.pi / 180.0\n",
    "    \n",
    "    N = a / math.sqrt(1 - e_sq * math.sin(lat_rad)**2)\n",
    "    \n",
    "    x = (N + alt) * math.cos(lat_rad) * math.cos(lon_rad)\n",
    "    y = (N + alt) * math.cos(lat_rad) * math.sin(lon_rad)\n",
    "    z = (N * (1 - e_sq) + alt) * math.sin(lat_rad)\n",
    "    \n",
    "    return (x, y, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDF for the conversion\n",
    "ecef_schema = StructType([\n",
    "    StructField(\"x\", DoubleType(), False),\n",
    "    StructField(\"y\", DoubleType(), False),\n",
    "    StructField(\"z\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ecef(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    try:\n",
    "        x, y, z = lat_lon_to_ecef(float(lat), float(lon))\n",
    "        return (x, y, z)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecef_udf = F.udf(convert_to_ecef, ecef_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to create ECEF coordinates\n",
    "df = df.withColumn(\"ecef\", ecef_udf(F.col(\"latitude\"), F.col(\"longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual coordinates\n",
    "df = df.withColumn(\"ecef_x\", F.col(\"ecef.x\"))\n",
    "df = df.withColumn(\"ecef_y\", F.col(\"ecef.y\"))\n",
    "df = df.withColumn(\"ecef_z\", F.col(\"ecef.z\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the temporary struct column\n",
    "df = df.drop(\"ecef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|ecef_x            |\n",
      "+------------------+\n",
      "|-5915153.491310677|\n",
      "|6176305.363394763 |\n",
      "|3341897.406283359 |\n",
      "|159804.00269604026|\n",
      "|1675260.9402652734|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"ecef_x\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature types for building the ML pipeline based on the transformation proposal\n",
    "numerical_features = [\n",
    "    'company_size', 'benefits_count', 'skills_count', 'responsibilities_count',\n",
    "    'job_posting_year', 'experience_min', 'experience_max', 'experience_avg',\n",
    "    'ecef_x', 'ecef_y', 'ecef_z', 'is_public'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'qualifications', 'work_type', 'preference', 'job_portal', 'sector', \n",
    "    'industry', 'company_state', 'ceo_gender'\n",
    "]\n",
    "\n",
    "text_features = [\n",
    "    'job_title', 'role', 'job_description'\n",
    "]\n",
    "\n",
    "cyclical_features = [\n",
    "    'month_sin', 'month_cos', 'day_sin', 'day_cos'\n",
    "]\n",
    "\n",
    "binary_features = [col for col in df.columns if col.startswith('has_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target is salary_avg (average of min and max salary)\n",
    "target = 'salary_avg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features to use in the model\n",
    "all_features = numerical_features + categorical_features + text_features + cyclical_features + binary_features\n",
    "selected_columns = all_features + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 42 columns for the ML pipeline\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "df_selected = df.select(selected_columns)\n",
    "print(f\"Selected {len(selected_columns)} columns for the ML pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company_size',\n",
       " 'benefits_count',\n",
       " 'skills_count',\n",
       " 'responsibilities_count',\n",
       " 'job_posting_year',\n",
       " 'experience_min',\n",
       " 'experience_max',\n",
       " 'experience_avg',\n",
       " 'ecef_x',\n",
       " 'ecef_y',\n",
       " 'ecef_z',\n",
       " 'is_public',\n",
       " 'qualifications',\n",
       " 'work_type',\n",
       " 'preference',\n",
       " 'job_portal',\n",
       " 'sector',\n",
       " 'industry',\n",
       " 'company_state',\n",
       " 'ceo_gender',\n",
       " 'job_title',\n",
       " 'role',\n",
       " 'job_description',\n",
       " 'month_sin',\n",
       " 'month_cos',\n",
       " 'day_sin',\n",
       " 'day_cos',\n",
       " 'has_health_insurance',\n",
       " 'has_dental',\n",
       " 'has_vision',\n",
       " 'has_401k',\n",
       " 'has_retirement',\n",
       " 'has_pto',\n",
       " 'has_paid_time_off',\n",
       " 'has_flexible',\n",
       " 'has_remote',\n",
       " 'has_bonus',\n",
       " 'has_education',\n",
       " 'has_training',\n",
       " 'has_insurance',\n",
       " 'has_life_insurance',\n",
       " 'salary_avg']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, \n",
    "    Tokenizer, StopWordsRemover, CountVectorizer, IDF,\n",
    "    StandardScaler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process categorical features\n",
    "categorical_indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", handleInvalid=\"skip\") \n",
    "                       for c in categorical_features]\n",
    "categorical_encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=f\"{indexer.getOutputCol()}_encoded\") \n",
    "                       for indexer in categorical_indexers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text features\n",
    "tokenizers = [Tokenizer(inputCol=c, outputCol=f\"{c}_tokens\") for c in text_features]\n",
    "remover = [StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=f\"{c}_filtered\") \n",
    "          for tokenizer, c in zip(tokenizers, text_features)]\n",
    "count_vectorizers = [CountVectorizer(inputCol=remover[i].getOutputCol(), outputCol=f\"{c}_counted\", minDF=5.0) \n",
    "                    for i, c in enumerate(text_features)]\n",
    "idfs = [IDF(inputCol=count_vectorizer.getOutputCol(), outputCol=f\"{c}_tfidf\") \n",
    "       for count_vectorizer, c in  zip(count_vectorizers, text_features)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature columns after transformations\n",
    "transformed_categorical = [encoder.getOutputCol() for encoder in categorical_encoders]\n",
    "transformed_text = [idf.getOutputCol() for idf in idfs]\n",
    "all_features = numerical_features + cyclical_features + binary_features + transformed_categorical + transformed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First assemble just the numerical features for scaling\n",
    "numerical_assembler = VectorAssembler(inputCols=numerical_features, outputCol=\"numerical_features\")\n",
    "numerical_scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Then assemble all features with the scaled numerical ones\n",
    "all_features_for_assembly = [\"scaled_numerical_features\"] + cyclical_features + transformed_categorical + transformed_text + binary_features\n",
    "assembler = VectorAssembler(inputCols=all_features_for_assembly, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline stages\n",
    "stages = categorical_indexers + categorical_encoders + tokenizers + remover + count_vectorizers + idfs\n",
    "stages.append(numerical_assembler)\n",
    "stages.append(numerical_scaler)\n",
    "stages.append(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pipeline with 31 stages\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "print(f\"Created pipeline with {len(stages)} stages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Pipeline and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the data\n",
    "pipeline_model = pipeline.fit(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "transformed_data = pipeline_model.transform(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitted and data transformed successfully\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns we need for ML\n",
    "ml_data = transformed_data.select(\"features\", F.col(target).alias(\"label\"))\n",
    "print(\"Pipeline fitted and data transformed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (70%) and test (30%) sets\n",
    "(train_data, test_data) = ml_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1111430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 475255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show sizes of train and test sets\n",
    "print(f\"Training set size: {train_data.count()}\")\n",
    "print(f\"Test set size: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'label']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save training data to HDFS in JSON format\n",
    "train_data.repartition(3) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"json\") \\\n",
    "    .save(\"project/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save test data to HDFS in JSON format\n",
    "test_data.repartition(2) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"json\") \\\n",
    "    .save(\"project/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
