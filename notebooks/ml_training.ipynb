{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/27 09:31:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/27 09:31:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/27 09:31:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/27 09:31:41 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/04/27 09:31:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Add here your team number\n",
    "team = 14\n",
    "# .master(\"local[*]\") \\\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(f\"Team {team} - spark ML Job Descriptions\")\\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"client\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Team 14 - spark ML Job Descriptions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb8546a5810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "input_schema = StructType([\n",
    "        StructField(\"features\", VectorUDT(), True), # Features are stored as vectors\n",
    "        StructField(\"label\", DoubleType(), True)    # Label is a double (salary_avg)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|(2920,[0,2,3,4,5,...|95500.0|\n",
      "|(2920,[0,2,3,4,5,...|74500.0|\n",
      "|(2920,[0,2,3,4,5,...|88000.0|\n",
      "|(2920,[0,2,3,4,5,...|77000.0|\n",
      "|(2920,[0,2,3,4,5,...|88000.0|\n",
      "|(2920,[0,2,3,4,5,...|89000.0|\n",
      "|(2920,[0,2,3,4,5,...|93500.0|\n",
      "|(2920,[0,2,3,4,5,...|74000.0|\n",
      "|(2920,[0,2,3,4,5,...|80500.0|\n",
      "|(2920,[0,2,3,4,5,...|88500.0|\n",
      "|(2920,[0,2,3,4,5,...|86500.0|\n",
      "|(2920,[0,2,3,4,5,...|85500.0|\n",
      "|(2920,[0,2,3,4,5,...|83000.0|\n",
      "|(2920,[0,2,3,4,5,...|72000.0|\n",
      "|(2920,[0,2,3,4,5,...|88000.0|\n",
      "|(2920,[0,2,3,4,5,...|78500.0|\n",
      "|(2920,[0,2,3,4,5,...|87500.0|\n",
      "|(2920,[0,2,3,4,5,...|76500.0|\n",
      "|(2920,[0,2,3,4,5,...|72500.0|\n",
      "|(2920,[0,2,3,4,5,...|75500.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.format(\"json\").schema(input_schema).load(\"project/data/train\")\n",
    "test_data = spark.read.format(\"json\").schema(input_schema).load(\"project/data/test\")\n",
    "\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=evaluator_rmse,\n",
    "    trainRatio=0.8,  # 80% train, 20% validation\n",
    "    parallelism=4    # safe to set >1 here if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TrainValidationSplit for Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/27 09:33:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/04/27 09:33:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Running TrainValidationSplit for Linear Regression...\")\n",
    "lr_tvs_model = tvs.fit(train_data)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LR Model Params: regParam=0.5, elasticNetParam=1.0\n"
     ]
    }
   ],
   "source": [
    "lr_best_model = lr_tvs_model.bestModel\n",
    "print(f\"Best LR Model Params: regParam={lr_best_model._java_obj.getRegParam()}, elasticNetParam={lr_best_model._java_obj.getElasticNetParam()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "gbt_grid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n",
    "        .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "        .addGrid(gbt.stepSize, [0.1, 0.05]) \\\n",
    "        .build()\n",
    "gbt_tsv = TrainValidationSplit(estimator=gbt,\n",
    "                          estimatorParamMaps=gbt_grid,\n",
    "                          evaluator=evaluator_rmse, # Use RMSE for tuning\n",
    "                          trainRatio=80,\n",
    "                          parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TrainValidationSplit for GBTRegressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 239:=> (5 + 2) / 9][Stage 241:>  (0 + 0) / 9][Stage 243:>  (0 + 0) / 9]  \r"
     ]
    }
   ],
   "source": [
    "print(\"Running TrainValidationSplit for GBTRegressor...\")\n",
    "gbt_tvs_model = gbt_tsv.fit(train_data)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbt_model = gbt_tvs_model.bestModel\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best maxDepth: {best_gbt_model._java_obj.getMaxDepth()}\")\n",
    "print(f\"Best maxIter: {best_gbt_model._java_obj.getMaxIter()}\")\n",
    "print(f\"Best stepSize: {best_gbt_model._java_obj.getStepSize()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
