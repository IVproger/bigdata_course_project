{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c251e1-9f46-4a5f-a23a-14f854e21537",
   "metadata": {},
   "source": [
    "# Column analysis\n",
    "\n",
    "This notebook is dedicated to column analysis prior to data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755451af-ac25-4573-b488-a5227f35fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/22 15:04:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/22 15:04:12 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/04/22 15:04:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 14\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Team {} - spark Column Analysis\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6748928-ed2b-4b17-bac5-490d2d73cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|team12_hive_proje...|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "|    team22_projectbd|\n",
      "|    team22_projectdb|\n",
      "|    team23_projectdb|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccc5533-aee6-4554-b981-1828b3eea0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'team14_projectdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382529ac-8f03-4cce-8dd0-8d35b1f8c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='job_descriptions_part', catalog='spark_catalog', namespace=['team14_projectdb'], description=None, tableType='EXTERNAL', isTemporary=False)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3ca267-308d-44e8-9e41-42e8d7b7d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- job_id: long (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      " |-- qualifications: string (nullable = true)\n",
      " |-- salary_range: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: decimal(9,6) (nullable = true)\n",
      " |-- longitude: decimal(9,6) (nullable = true)\n",
      " |-- company_size: integer (nullable = true)\n",
      " |-- job_posting_date: date (nullable = true)\n",
      " |-- contact_person: string (nullable = true)\n",
      " |-- preference: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- job_portal: string (nullable = true)\n",
      " |-- job_description: string (nullable = true)\n",
      " |-- benefits: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      " |-- responsibilities: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_profile: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = 'job_descriptions_part'\n",
    "df = spark.read.format(\"avro\").table(f'{db}.{table_name}')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca81437e-ac33-4f9f-9356-318fce38afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756a985a-3822-4ea8-abb4-f9dd8829c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/22 15:04:34 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1173529, 'job_id': 364241521459786, 'experience': '4 to 11 Years', 'qualifications': 'BA', 'salary_range': '$62K-$118K', 'location': 'Antananarivo', 'country': 'Madagascar', 'latitude': Decimal('-18.879200'), 'longitude': Decimal('46.845100'), 'company_size': 121206, 'job_posting_date': datetime.date(2022, 1, 31), 'contact_person': 'Calvin Thompson', 'preference': 'Female', 'contact': '472.993.7608x09238', 'job_title': 'Project Coordinator', 'role': 'Construction Project Coordinator', 'job_portal': 'Jobs2Careers', 'job_description': 'Construction Project Coordinators assist in managing construction projects, handling documentation, budget tracking, and coordinating subcontractors.', 'benefits': \"{'Casual Dress Code, Social and Recreational Activities, Employee Referral Programs, Health and Wellness Facilities, Life and Disability Insurance'}\", 'skills': 'Construction project management Building codes and regulations knowledge Budgeting and cost control Construction scheduling Contractor and subcontractor coordination', 'responsibilities': 'Coordinate construction projects, including permits, subcontractors, and materials. Monitor project progress and timelines. Assist in budgeting and cost control.', 'company_name': 'Sysco', 'company_profile': '{\"Sector\":\"Food and Beverage\",\"Industry\":\"Wholesalers: Food and Grocery\",\"City\":\"Houston\",\"State\":\"Texas\",\"Zip\":\"77077\",\"Website\":\"www.sysco.com\",\"Ticker\":\"SYY\",\"CEO\":\"Kevin P. Hourican\"}', 'work_type': 'Intern'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "first_row_dict = df.limit(1).toPandas().iloc[0].to_dict()\n",
    "print(first_row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08223e9d-da2d-4384-9f0e-bdf28ad4cfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: qualifications\n",
      "10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: location\n",
      "214\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: country\n",
      "216\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: work_type\n",
      "5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: preference\n",
      "3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: job_title\n",
      "147\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: role\n",
      "376\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: job_portal\n",
      "16\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: company_name\n",
      "888\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, udf, from_json, when, regexp_replace\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import json\n",
    "\n",
    "categorical = ['qualifications', 'location', 'country', 'work_type', 'preference', 'job_title', 'role', 'job_portal', 'company_name']\n",
    "\n",
    "for column in categorical:\n",
    "    count = df.select(countDistinct(col(column))).collect()[0][0]\n",
    "    print(f\"Column: {column}\")\n",
    "    print(count)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c229ac6-29a4-4c33-8a04-8bac2d0af0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Company Profile JSONs: 1608618\n",
      "Invalid Company Profile JSONs: 7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def is_valid_json(s):\n",
    "    try:\n",
    "        json.loads(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "is_valid_json_udf = udf(is_valid_json)\n",
    "\n",
    "# Add column indicating if JSON is valid\n",
    "df = df.withColumn(\"is_valid_json\", is_valid_json_udf(col(\"company_profile\")))\n",
    "\n",
    "# Count valid and invalid JSON rows\n",
    "valid_json_count = df.filter(col(\"is_valid_json\") == True).count()\n",
    "invalid_json_count = df.filter(col(\"is_valid_json\") == False).count()\n",
    "\n",
    "print(f\"Valid Company Profile JSONs: {valid_json_count}\")\n",
    "print(f\"Invalid Company Profile JSONs: {invalid_json_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30df660-5343-44be-a91c-58ee3077d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.filter(col(\"is_valid_json\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99564e4c-58ff-4a68-a480-9cb1840c8e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------------------------------------+------------+--------+-----+----------------------------------+------+-------------------+\n",
      "|Sector                       |Industry                                |City        |State   |Zip  |Website                           |Ticker|CEO                |\n",
      "+-----------------------------+----------------------------------------+------------+--------+-----+----------------------------------+------+-------------------+\n",
      "|Food and Beverage            |Food Manufacturing                      |Chicago     |IL      |60601|https://www.kraftheinzcompany.com/|KHC   |Miguel Patricio    |\n",
      "|Financial Services           |Insurance: Property and Casualty (Stock)|Jacksonville|Florida |32204|www.fnf.com                       |FNF   |Mike Nolan         |\n",
      "|Transportation/Infrastructure|Transportation/Infrastructure           |Melbourne   |VIC     |3000 |https://www.transurban.com/       |TCL   |Scott Charlton     |\n",
      "|Automotive                   |Motor Vehicles & Parts                  |Auburn Hills|Michigan|48326|www.autoliv.com                   |ALV   |Mikael Bratt       |\n",
      "|Transportation               |Trucking, Truck Leasing                 |Lowell      |Arkansas|72745|www.jbhunt.com                    |JBHT  |John N. Roberts Iii|\n",
      "+-----------------------------+----------------------------------------+------------+--------+-----+----------------------------------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Company_Profile_Cleaned\", regexp_replace(col(\"company_profile\"), \"'\", '\"'))\n",
    "\n",
    "company_profile_schema = StructType([\n",
    "    StructField(\"Sector\", StringType(), True),\n",
    "    StructField(\"Industry\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", StringType(), True),\n",
    "    StructField(\"Website\", StringType(), True),\n",
    "    StructField(\"Ticker\", StringType(), True),\n",
    "    StructField(\"CEO\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_valid = df.withColumn(\n",
    "    \"CompanyProfileParsed\",\n",
    "    from_json(col(\"Company_Profile_Cleaned\"), company_profile_schema)\n",
    ")\n",
    "\n",
    "company_profile_df = df_valid.select(\"CompanyProfileParsed.*\")\n",
    "company_profile_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cb8c91-b8e1-4dfc-9d5e-b2b2fee33acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Sector\n",
      "Unique values: 204\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Industry\n",
      "Unique values: 203\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: City\n",
      "Unique values: 341\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: State\n",
      "Unique values: 98\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Zip\n",
      "Unique values: 491\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Website\n",
      "Unique values: 869\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Ticker\n",
      "Unique values: 807\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:===================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: CEO\n",
      "Unique values: 825\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for column in company_profile_df.columns:\n",
    "    count = company_profile_df.select(countDistinct(col(column))).collect()[0][0]\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Unique values: {count}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c201c81-2abb-4275-92ce-89e71ceda7af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gender_guesser'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgender_guesser\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgender\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m udf, col\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringType\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gender_guesser'"
     ]
    }
   ],
   "source": [
    "import gender_guesser.detector as gender\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_ceo_gender(name):\n",
    "    if name is None or name.strip() == \"\":\n",
    "        return \"unknown\"\n",
    "    first_name = name.split()[0]\n",
    "    d = gender.Detector()\n",
    "    result = d.get_gender(first_name)\n",
    "    if result in [\"male\", \"mostly_male\"]:\n",
    "        return \"male\"\n",
    "    elif result in [\"female\", \"mostly_female\"]:\n",
    "        return \"female\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "get_ceo_gender_udf = udf(get_ceo_gender, StringType())\n",
    "\n",
    "company_profile_df = company_profile_df.withColumn(\"CEO_Gender\", get_ceo_gender_udf(col(\"CEO\")))\n",
    "\n",
    "gender_counts = company_profile_df.groupBy(\"CEO_Gender\").count()\n",
    "\n",
    "print(\"Gender counts for CEOs:\")\n",
    "gender_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f169a0-2d62-4202-8330-83cc54ca902c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
